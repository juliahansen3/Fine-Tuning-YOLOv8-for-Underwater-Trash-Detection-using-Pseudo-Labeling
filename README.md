# Fine-Tuning-YOLOv8-for-Underwater-Trash-Detection-using-Pseudo-Labeling
## Problem Statement:
So much plastic ends up in our marine ecosystems, harming both marine organisms and humans. This project makes use of the J-EDI dataset of deep sea marine debris and a technique called Pseudo Labeling to attempt to make use of the vast amount of unlabeled data in improving model accuracy of the type of object present in the water. Pseudo Labeling is a technique also known as semi supervised learning where a trained model makes predictions on unlabeled data and that data is iteratively added to the training dataset. We used YOLO v8 for this project.
## Hypothesis:
We hypothesized that using pseudo labeling on unlabeled marine debris images would improve our model from a baseline accuracy of around 88%. We first compared confidence thresholds and hypothesized that increasing the confidence threshold would improve the model's results. We then compared the amount of unlabeled data used at each iteration and hypothesized that the more data that was added, the better our model's results would be.
## Data:
We used the JAMSTEC E-library of Deep-sea Images (J-EDI) database. The labeled dataset that is created from this database contains 7212 images total split into 6065 train images and 1147 validation images. The images are labeled by 22 classes, including the rov equipment used to collect the data, different kinds of trash and natural elements. This dataset is formatted as video frames with annotations in a .json file, and we used Roboflow to create a .yaml file for YOLO and reorganize the data to have 5048 training images, 1443 validation images, and 721 test images. 
## Method:
The first controlled pseudo labeling experiment was fine-tuning YOLO v8 using a range of confidence thresholds - 0.9, 0.8, 0.7, 0.6, and 0.5. This experiment would use a constant amount of data (100 images) each iteration, but adjust confidence thresholds when adding pseudo data. Only the unlabeled images that were classified with a confidence greater than said threshold would be fed back into YOLOv8 for re-fine-tuning. Through this experiment, it could be investigated if there is a certain confidence threshold that works the best for pseudo labeling. 

The second controlled pseudo labeling experiment was run to vary the percentage of the train dataset being used to re-fine-tune - 100, 200, 300, or 400 images of the test dataset (which has a total of 721 images).  Using a constant confidence threshold of 0.25 for each iteration, the percentage of data added as pseudo labels would be adjusted. Through this experiment, it could be investigated if there is a tradeoff between the ratio of unlabeled to labeled data used for the fine-tuning process. 
## First Set of Results:
In the data folder we have the plots that examine the changes in recall, precision, and accuracy as we 1) vary the confidence threshold and 2) vary the amount of unlabelled data being added. We can see that there does not appear to be one confidence threshold that improves the results the most. The accuracy does not vary much and the precision and recall values do not vary in a linear pattern. The same is true for our second test; adding more data did not improve the results.
## Second Set of Results:
Because the database contains 40 thousand hours of video, I tried to take unlabelled data from the database to expand our test set and add almost 2,000 new images instead of just 400 to see if that would improve results. I also used a slightly different version of the dataset that resulted in worse performance to see if pseudo labelling would have more of an impact on a model with a lower accuracy baseline value. Although there were some iterations where performance increased and in validation the model was able to detect previously unseen items with higher confidence, on average the model still didn't seem to show signs of improving. Upon discussion with our advisor at the end of the project, the next step is to exponentially increase the number of images added at each iteration in order to see a difference in the model's results since only adding a thousand images isn't enough compared to the size of the dataset. It could be that even if we logarithmically increase our data, pseudo labelling doesn't have any impact on model performance in this specific task. 
