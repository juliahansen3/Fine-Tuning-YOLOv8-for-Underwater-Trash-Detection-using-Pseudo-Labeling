# Fine-Tuning-YOLOv8-for-Underwater-Trash-Detection-using-Pseudo-Labeling
## Problem Statement:
So much plastic ends up in our marine ecosystems, harming both marine organisms and humans. An autonomous underwater vehicle could detect and capture plastic debris. This project makes use of the J-EDI dataset of deep sea marine debris and a technique called Pseudo Labeling to attempt to make use of the vast amount of unlabeled data in improving model accuracy of detecting plastic debris. Pseudo Labeling is a technique also known as semi supervised learning where a trained model makes predictions on unlabeled data and that data is iteratively added to the training dataset. My partner and I used YOLO v8 for this college project.
## Hypothesis:
We began by using YOLO v8 which already yielded a relatively good results (88% accuracy). We hypothesized that using pseudo labeling on unlabeled marine debris images would improve our model. We first compared confidence thresholds and hypothesized that increasing the confidence threshold would improve the model's results. We then compared the amount of unlabeled data used at each iteration and hypothesized that the more data that was added, the better our model's results would be.
## Data:
We used the JAMSTEC E-library of Deep-sea Images (J-EDI) database. The labeled dataset that is created from this database contains 7212 images total split into 6065 train images and 1147 validation images. The images are labeled by 22 classes, including the rov equipment used to collect the data, different kinds of trash and natural elements. This dataset is formatted as video frames with annotations in a .json file, and we used Roboflow to create a .yaml file for YOLO and reorganize the data to have 5048 training images, 1443 validation images, and 721 test images. 
## Method:
The first controlled pseudo labeling experiment was fine-tuning YOLO v8 using a range of confidence thresholds - 0.9, 0.8, 0.7, 0.6, and 0.5. This experiment would use a constant amount of data (100 images) for each iteration, but would adjust confidence thresholds when adding pseudo data. Only the unlabeled images that were classified with a confidence greater than said threshold would be fed back into YOLOv8 for re-fine-tuning. Through this experiment, it could be investigated if there is a certain confidence threshold that works the best for pseudo labeling. 

The second controlled pseudo labeling experiment was run to vary the percentage of the train dataset being used to re-fine-tune - 100, 200, 300, or 400 images of the test dataset (which has a total of 721 images).  Using a constant confidence threshold of 0.25 for each iteration, the percentage of data added as pseudo labels would be adjusted. Through this experiment, it could be investigated if there is a tradeoff between the ratio of unlabeled to labeled data used for the fine-tuning process. 
## First Set of Results:
In the data folder we have the plots that examine the changes in recall, precision, and accuracy as we 1) vary the confidence threshold and 2) vary the amount of unlabelled data being added. We can see that there does not appear to be one confidence threshold that improves the results the most. The accuracy does not vary much and the precision and recall values do not vary in a linear pattern. The same is true for our second test; adding more data did not improve the results.
## Second Set of Results:
I continued the project by trying to use more data from the large database that contains 40 thousand hours of video. I expanded the test set by almost 2,000 new images. Although there were some iterations where performance increased, on average the model did not improve. At this point I believe the dataset is still too small to produce meaningful improvements in accuracy. The next step would be to exponentially increase the number of images added at each iteration.
